#!/usr/bin/env python
# coding: utf-8

# ##                                      "**data_cleaning**"

# ## importing libraries

# In[5]:


import numpy as np
import pandas as pd
import statsmodels.api as sm
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()


# ## loading raw data

# In[6]:


raw_data = pd.read_csv('https://cdn.fs.teachablecdn.com/dg64Ln4QtiFVVmTSLHOD')
raw_data.head()


# ## preprocessing

# ### exploring descriptive statistics of raw_data

# In[7]:


raw_data.describe(include = 'all')


# ### droping umimportant variables
# 

# In[8]:


data = raw_data.drop(['Model'],axis = 1)
data.describe(include = 'all')


# ### dealing with missing values

# In[9]:


data.isnull()


# In[10]:


data.isnull().sum()


# ### data with no missing values

# In[11]:


data_no_mv = data.dropna(axis = 0)


# In[12]:


data_no_mv


# ### Exploring probability distribution functions (PDF's)

# In[13]:


sns.distplot(data_no_mv['Price'])


# ### dealing with outliers

# In[14]:


q = data_no_mv['Price'].quantile(0.99)


# In[15]:


data_1 = data_no_mv[data_no_mv['Price']<q]


# In[16]:


data_1.describe(include = 'all')


# In[17]:


sns.distplot(data_1['Price'])


# In[18]:


sns.distplot(data_1['Mileage'])


# In[19]:


q = data_1['Mileage'].quantile(0.99)
data_2 = data_1[data_1['Mileage']<q]


# In[20]:


sns.distplot(data_2['Mileage'])


# In[21]:


sns.distplot(data_2['EngineV'])


# In[22]:


data_3 = data_2[data_2['EngineV']<6.5]


# In[23]:


sns.distplot(data_3['EngineV'])


# In[24]:


sns.distplot(data_3['Year'])


# In[25]:


q = data_3['Year'].quantile(0.1)
data_4 = data_3[data_3['Year']>q]


# In[26]:


sns.distplot(data_4['Year'])


# ### defining cleaned data

# In[27]:


data_cleaned = data_4.reset_index(drop =True)


# In[28]:


data_cleaned.describe(include='all')


# In[29]:


## checking ols assumptions


# In[30]:


f , (ax1, ax2, ax3) = plt.subplots(1, 3, sharey = True, figsize = (15,3))
ax1.scatter(data_cleaned['Year'],data_cleaned['Price'])
ax1.set_title('price and year')
ax2.scatter(data_cleaned['EngineV'],data_cleaned['Price'])
ax2.set_title('price and engine volume')
ax3.scatter(data_cleaned['Mileage'],data_cleaned['Price'])
ax3.set_title('price and mileage')
plt.show()


# In[31]:


sns.distplot(data_cleaned['Price'])


# In[32]:


log_price = np.log(data_cleaned['Price'])


# In[33]:


data_cleaned['log_price'] = log_price


# In[34]:


## checking multicolinearity [if vif between 1-5 acceptable, if more than 7 then drop]


# In[35]:


from statsmodels.stats.outliers_influence import variance_inflation_factor


# In[36]:


variables = data_cleaned[['Mileage','Price','EngineV','Year']]
vif = pd.DataFrame()
vif['VIF'] = [variance_inflation_factor(variables.values, i) for i in range(variables.shape[1])]
vif['features'] = variables.columns


# In[37]:


vif


# In[38]:


data_no_multicollinearity = data_cleaned.drop(['Price'],axis = 1)


# In[39]:


data_no_multicollinearity


# In[40]:


f, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey = True, figsize = (15,3))
ax1.scatter(data_cleaned['Year'],data_cleaned['log_price'])
ax1.set_title('log price and year')
ax2.scatter(data_cleaned['EngineV'],data_cleaned['log_price'])
ax2.set_title('log price and engine volume')
ax3.scatter(data_cleaned['Mileage'],data_cleaned['log_price'])
ax3.set_title('log price and mileage')
plt.show()


# In[41]:


data_no_multicollinearity.drop(['Year'], axis = 1)


# ### creating dummies

# In[42]:


data_with_dummies = pd.get_dummies(data_no_multicollinearity, drop_first = True)


# In[43]:


data_with_dummies


# ### Rearranging data according to our comfort

# In[44]:


data_with_dummies.columns.values


# In[45]:


cols = ['log_price','Mileage', 'EngineV', 'Year',  'Brand_BMW',
       'Brand_Mercedes-Benz', 'Brand_Mitsubishi', 'Brand_Renault',
       'Brand_Toyota', 'Brand_Volkswagen', 'Body_hatch', 'Body_other',
       'Body_sedan', 'Body_vagon', 'Body_van', 'Engine Type_Gas',
       'Engine Type_Other', 'Engine Type_Petrol', 'Registration_yes']


# In[46]:


data_preprocessed = data_with_dummies[cols]


# In[47]:


data_preprocessed


# ## preparing Linear regresion model

# ### Declaring variables

# In[48]:


targets = data_preprocessed['log_price']
inputs = data_preprocessed.drop(['log_price'], axis = 1)


# ### scaling inputs

# In[49]:


from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(inputs)


# In[50]:


inputs_scaled = scaler.transform(inputs)


# ### train test split

# In[51]:


from sklearn.model_selection import train_test_split


# In[52]:


x_train,x_test, y_train,y_test = train_test_split(inputs_scaled, targets, test_size = 0.2, random_state = 365)


# ## create regression

# In[53]:


reg = LinearRegression()
reg.fit(x_train,y_train)


# In[54]:


y_hat = reg.predict(x_train)


# In[55]:


plt.scatter(y_train,y_hat)
plt.xlabel('y_train (targets)', size = 15)
plt.ylabel('y_hat (predictions)', size = 15)
plt.xlim(6,13)
plt.ylim(6,13)
plt.show()


# ### Residuals PDF

# In[53]:


sns.distplot(y_train - y_hat)
plt.title('residuals PDF', size =18)


# ### R squared

# In[54]:


reg.score(x_train, y_train)


# ### Weights and Bias

# In[55]:


bias = reg.intercept_
bias


# In[56]:


weights = reg.coef_
weights


# In[57]:


summary_table = pd.DataFrame(inputs.columns.values, columns = ['Features'])
summary_table['Weights'] = weights
summary_table


# In[58]:


data_cleaned['Brand'].unique()


# ## TESTING

# In[59]:


y_hat_test = reg.predict(x_test)


# In[60]:


plt.scatter(y_test, y_hat_test, alpha = 0.2)
plt.xlabel('targets (y_test)', size = 15)
plt.ylabel('predictions (y_hat_test)', size = 15)
plt.xlim(6,13)
plt.ylim(6,13)
plt.show()


# ### dataframe performance

# In[63]:


df_pf = pd.DataFrame(np.exp(y_hat_test), columns =['predictions'])
df_pf.head()


# In[72]:


y_test = y_test.reset_index(drop = True)


# In[73]:


df_pf['Targets'] = np.exp(y_test)
df_pf


# In[76]:


df_pf['Residuals'] = df_pf['Targets'] - df_pf['predictions']
df_pf['difference%'] = np.absolute(df_pf['Residuals']/df_pf['Targets']*100)
df_pf


# In[77]:


df_pf.describe()


# In[81]:


pd.options.display.max_rows = 999
pd.set_option('display.float_format', lambda x: '%.2f' %x)
df_pf.sort_values(by=['difference%'])

